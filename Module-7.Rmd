---
title: "Module-7"
author: "Nicole Merullo"
date: "2023-09-17"
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
---
```{r titimonkey, echo=FALSE, fig.align='center', fig.cap='Cinnamon,the titi monkey from the Denver Zoo', out.width='25%'}
knitr::include_graphics('~/Documents/BU/AN_588/AN588-Modules-nmer/titimonkey1.png')
```
```{r setup, include=FALSE}
library(sciplot)
```

## Central Tendency and Variance

Central tendency are where things tend to go- different way of measuring averages. 
Just pasting in the terms and definitions from the instructions for reference:

**Population** = includes all of the elements from a set of data (e.g., all of the vervet monkeys in the world) = N
**Sample** = one or more observations from a population (e.g., the set of vervets living in South Africa, the set of vervet skeletons found in a museum) = n
**Parameter** = a measurable characteristic of a population (e.g., the mean value of the femur length of all vervets)
**Statistic** = a measurable characteristic about a sample (e.g., the mean femur length of vervet monkey femurs found at the American Museum of Natural History)

Measures of Central Tendency 
**Mode** = most common measurement of values observed
**Median** = middle value in a rank ordered series of values
**Mean** = the sum of measured values divided by n = the average or the arithmetic mean
**Harmonic mean** = the reciprocal of the average of the reciprocals of a set of values.
**Geometric mean** = a measure of central tendency for processes that are multiplicative rather than additive = the nth root of the product of the values (for the mathematically inclindes, it also = the antilog of the averaged log values).

### Challenge 1 Measures of Central Tendency

write a function to determine the geometric mean of the values in a vector.

```{r Challenge1-1, echo=TRUE}
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 25, 50, 100, 200, 1000)
gm1 <- function(x) {
    prod(x)^(1/length(x))
}
gm1(x)
```

So this is the products of each integer in the vector raised to the exponent 1/# of elements in the vector
```{r Challenge1-2, echo=TRUE}
p <- prod(x) #multiplying all of these elements together
p
l <- length(x)
l
p^(1/l)
```
Another way of making this function:

```{r Challenge1-3, echo=TRUE}
gm2 <- function(x) {
    exp(mean(log(x)))
}
gm2(x)
```

Same answer. exponential of the mean of the logarithm of x

```{r Challenge1-4, echo=TRUE}
x1 <- log(x)
x1
x2 <- mean(x1)
x2
exp(x2)
```

What happens if you have NAs or zeros or negative numbers in the vector?

Negative:

```{r Challenge1-5, echo=TRUE}
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 25, -50, 100, 200, 1000)
gm1 <- function(x) {
    prod(x)^(1/length(x))
}
gm1(x)
```

Geometric mean only works with positive numbers

NA Value:

```{r Challenge1-5, echo=TRUE}
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 25, 50, 100, 200, 1000, NA)
gm1 <- function(x) {
    prod(x, na.rm = TRUE)^(1/length(x))
}
gm1(x)
```
This gives a different answer than before because the length of the vector is longer but I was able to remove the NA value in the prod function.

### Challenge 2 Measures of Spread

Copying from the instructions for reference:

A measure of spread or variability in a dataset is one of the most important summary statistics to calculate. The range (min to max) is one measure of spread as is the interquartile range (25th to 75th quartile). As we’ve seen, these are returned by the summary() function.

We commonly characterize spread, however, in terms of the **deviation of values from the mean**. One such measure is the sum of squares…

**sum of squares** = the sum of the squared deviations of a set of values from the mean

[Why do we use the sum of the squared deviations of values from the mean rather than just the sum of deviations? Because the latter would simply be ZERO.] *one would be positive and one would be negative...?*

Write a function to calculate the sum of squares for a vector

```{r Challenge2-1, echo=TRUE}
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 25, 50, 100, 200, 1000)
mean(x) #first calculate the mean from which we will find the deviations
x - mean(x) #finding the deviations from the mean (each element in the vector to/from the mean)
(x - mean(x))^2 #squaring the deviations (each one)
sum((x - mean(x))^2) #summing the squared deviations
#and here is the function:
ss1 <- function(x) {
    sum((x - mean(x))^2)
}
ss1(x)
```

Alternative:

```{r Challenge2-2, echo=TRUE}
ss2 <- function(x) {
    sum(x^2) - length(x) * mean(x)^2
}
ss2(x)
```

And another without having to use mean():

```{r Challenge2-3, echo=TRUE}
ss3 <- function(x) {
    sum(x^2) - (sum(x))^2/length(x)
}
ss3(x)
```

Sum of squares gets bigger and bigger with more data points. To compare with different data sets then we need to look at the average deviation (i.e., how much each point diverges from the mean, on average) instead of the sum of squares. This is the **mean squared deviation** and is the definition of variance in a dataset. **Basically this is the average of the sum of squares.**

**population variance** = sum of squares / n

This works out to the following:

```{r Challenge2-4, echo=TRUE}
pop_v <- function(x) {
    sum((x - mean(x))^2)/length(x)
}
pop_v(x)
```

**sample variance** removes bias to estimate the population variance = sum of squares / (n-1)

Copied from the instructions to explain why it is n-1:
In this formula, n - 1 is the number of degrees of freedom implied by the sample. The degrees of freedom is the number of values used to calculate a sample statistic that are free to vary. We used n observations to calculate the mean of our sample, and that implies n - 1 degrees of freedom. We use that statistic about our sample as an estimate of the population mean, which is used to derive an estimate of the population variance.

### Challenge 3

Write a function to calculate the variance for a vector of values representing a sample of measurements. Compare the results of your function to the built-in function, var(), which calculates sample variance.

```{r Challenge3-1, echo=TRUE}
sample_v <- function(x) {
    sum((x - mean(x))^2)/(length(x) - 1) #added the degree of freedom
}
sample_v(x)
```

number is now larger than before just to note

trying the var() function - not looking at documentation just going to send it this first time

```{r Challenge3-2, echo=TRUE}
var(x)
```

Ok got the same answer!

How does Sample Variance compare to Population Variance? What happens to the sample variance as sample size increases?

```{r Challenge3-3, echo=TRUE}
y <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 25, 50, 100, 200, 300, 400, 1000)
var(y)
```

It gets smaller

Testing population variance with y vector (more elements)

```{r Challenge3-3, echo=TRUE}
pop_v(y)
sum((y - mean(y))^2)/length(y)
```

It is also smaller than x but it's also smaller than sample variance (just like it was with the x vector)

For a random variable, how is variance related to sample size? Let’s explore this…

[1] plot
[2] Create a random variable drawn from a normal distribution using the rnorm() function. Let’s use for loops to do this for samples of size 5, 10, 15… up to 50, with 50 replicates at each size. The structure for for loops is…

for (counter in vector){code to run}
```{r Challenge3-4, echo=TRUE}
plot(c(0, 50), c(0, 15), type = "n", xlab = "Sample Size", ylab = "Variance")
for (n in seq(5, 50, 5)) # samples of 5, 10, 15... for (n in seq(5, 50,
for (n in seq(5, 50, 5)) # samples of 5, 10, 15... 5)) # samples of 5, 10,
for (n in seq(5, 50, 5)) # samples of 5, 10, 15... 15...
{
    for (i in 1:50) # 50 replicates for (i in 1:50) # 50 replicates
    {
        x <- rnorm(n, mean = 10, sd = 2)
        points(n, var(x))
    }
}
```

**Standard Deviation** is simply the square root of the variance (taking the sum of squares to the population variance by averaging it and now square rooting it)

```{r Challenge3-5, echo=TRUE}
pop_sd <- function(x) {
    sqrt(pop_v(x))
}
pop_sd(x)
```

Alternatively with sample variance:

```{r Challenge3-6, echo=TRUE}
sample_sd <- function(x) {
    sqrt(sample_v(x))
}
sample_sd(x)
```

My answers are off from the instructions but I cannot identify where the issue is. Will have to revisit.

### Challenge 4 Using Measures of Spread

Copied from the instructions for reference:
We would also like to have an idea, based on characteristics of our sample, how reliable or unreliable our estimates of population parameters based on those samples are. In general, we would expect such a measure of uncertainty, or error, to increase with the variability in our sample (estimates with high variability are more uncertain) and to decrease as we sample more. That is, it should be proportional to the ratio of variance to sample size.

Also, ideally, the units for our estimate of error should be the same as those of our original measurements. Since the ratio above would be expressed in square units (since sample size is dimensionless), we can take the square root to express it in units.

The standard error of the mean, based on a sample, can thus be defined as follows:

**SE mean** = square root of the average sample variance

or

**SE mean** = square root of (sample variance / number of observations)

or

**SE mean** = (sample standard deviation) / square root of (number of observations)

Write a function of SE mean

```{r Challenge4-1, echo=TRUE}
SE1 <- function(x){
  sqrt(var(x)/length(x))
}
SE1(x)
```

Once again my answers are off and it doesn't match up whether I change my x vector back to the original or not. It's closer as it is from the plot so I'm going to leave it.

sciplot has a standard error function

```{r Challenge4-2, echo=TRUE}
se(x)
```

## Calculating Confidence Intervals using Standard Errors
Pasting most of this from the instructions for reference:

A **confidence interval** shows the likely range of values into which an estimate would fall if the sampling exercise were to be repeated

We can talk about different confidence intervals (e.g., 50%, 95%, 99%), and the higher the confidence we want to have, the wider the interval will be.

The 95% confidence interval, then, describes the range of values into which a statistic, calculated based on a repeated sample, would be expected to fall 95% of the time.  We typically estimate confidence intervals with respect to specific theoretical distributions (e.g., normal, Poisson, Student’s t, F) and different characteristics about our sample (e.g., mean, standard deviation, degrees of freedom).

For example, suppose we wanted to calculate a 95% confidence interval around our estimate of the mean for a particular set of observations, assuming those data reflect a random variable that is normally distributed and that our observations are independent. We would simply find the values corresponding to the numbers of standard errors away from the mean our statistic would be expected to fall 95% of the time.

We can calculate this by multiplying our estimate of the standard error by the quantile normal (qnorm()) function. Basically, we give the qnorm() function a quantile, and it returns the value of X below which that proportion of the cumulative probability function falls. For example, qnorm(0.025, mean=0, sd=1) tells us the number of standard deviations away from the mean that correspond with up to 2.5% of of the normal distribution with mean=0 and sd=1. qnorm(0.975, mean=0, sd=1) tells us the number of standard deviations up to which 97.5% of observations should fall.

Let’s take a quick look at the NORMAL DISTRIBUTION. Here, we use the rnorm() function to sample 10000 numbers from a normal distribution with mean = 0 and standard deviation = 1. [I am using set.seed() here so that each time I run this function, I return the same set of random numbers.]

