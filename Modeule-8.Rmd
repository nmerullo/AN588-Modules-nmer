---
title: "Module-8"
author: "Nicole Merullo"
date: "2023-09-24"
output: html_document
---

```{r setup, include=FALSE}
library(manipulate)
library(ggplot2)
```

## Probability
The term probability is applied to population level variables that describe the magnitude of chance associated with particular observations or event. Probabilities summarize the relative frequencies of possible outcomes. Probabilities are properties of distributions. Probabilities vary between zero and one. Outcomes that are impossible have Pr = 0, those that are certain have Pr = 1.

Example: if we roll a (fair) die, there are 6 possible outcomes, each has a probability of occurring of 1 in 6. This is referred to as a frequentist or classical way of thinking about the probability of different outcomes… the relative frequency with which an event occurs over numerous identical, objective trials.

Example: Rolling a Die
We will use the {manipulate} package and the sample() function to explore the effects of sample size on estimates of the probability of different outcomes. The probability of each outcome (rolling a “1”, “2”,…, “6”) is 1 in 6, but our estimate of the probability of each possible outcome will change with sample size.

```{r Probability-1, echo=TRUE}
outcomes <- c(1, 2, 3, 4, 5, 6) #facets on a die
manipulate(hist(sample(outcomes, n, replace = TRUE), breaks = c(0.5, 1.5, 2.5,
    3.5, 4.5, 5.5, 6.5), probability = TRUE, main = paste("Histogram of Outcomes of ",
    n, " Die Rolls", sep = ""), xlab = "roll", ylab = "probability"), n = slider(0,
    10000, initial = 100, step = 100)) 
```

sample() takes a sample of the data (first argument, in this case the vector 'outcomes') and samples it n number of times with or without replacement (putting back or not the elements)
hist() is building the histogram with the data being the sample() results, breaks being defined as between the vector elements, freq not defined but probability is so not defaulting to TRUE. 

Everytime we run the code it changes becausw it is simulating 100 die rolls each time

## Challenge 1: Dice Rolling

Write a function to simulate rolling a die where you pass the number of rolls as an argument. Then, use your function to simulate rolling two dice 1000 times and take the sum of the rolls. Plot a histogram of those results.

```{r Challenge1-1, echo=TRUE}
roll <- function(x){
  sample(1:6, x, replace=TRUE)
}
nrolls <- 1000
hist(roll(nrolls), breaks = c(0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5), probability = TRUE, main = "Probability of 1000 Dice Rolls", xlab = "roll", ylab = "probability") 
twodice <- roll(nrolls) + roll(nrolls)
hist(twodice, breaks = c(1.5:12.5), probability = TRUE, main = "Probability of 2 Dice", xlab = "roll", ylab = "probability")
```

## Rules of Probability
1. **Pr (+)**= Probability that something occurs = 1

2. **Pr (∅)** = Probability that nothing occurs = 0

3. **Pr (A)** = Probability that a particular event A occurs (prob of A happening)

0 ≤ Pr (A) ≤ 1 = Probability of A happening is between 0 and 1

4. **Pr (A ⋃ B)** = Probability that a particular event A or a particular event B occurs = UNION (probability of A OR B happening)

Pr (A ⋃ B) = Pr (A) + Pr (B) - Pr (A ⋂ B) (Prob of A plus Prob of B minus Prob of A AND B)

If event A and B are mutually exclusive, then this simplifies to Pr (A) + Pr (B)

5. **Pr (A ⋂ B)** = Probability that both A and B occur simultaneously = INTERSECTION (Probability of A AND B happening)

Pr (A ⋂ B) = Pr (A | B) × Pr (B) = Pr (B | A) × Pr (A) 

where the pipe operator ( | ) can be read as “given”. (dependent on)

If the 2 events are independent (i.e., if the probability of one does not depend on the probability of the other), then Pr (A ⋂ B) simplifies to…

Pr (A) × Pr (B)

If Pr (A ⋂ B) = 0, then we say the events are mutually exclusive (e.g., you cannot have a die roll be 1 and 2)

6. **Pr (Ā)** = Probability of the complement of A (i.e., not A) = 1 - Pr (A) (everything BUT the probability of A)

7. Conditional Probability is the probability of an event occuring after taking into account the occurrence of another event, i.e., one event is conditioned on the occurrence of a different event.

For example, the probability of a die coming up as a “1” given that we know the die came up as an odd number (“1”, “3”, or “5”).

**Pr (A | B) = Pr (A ⋂ B) ÷ Pr (B)**

If event A and event B are independent, then Pr (A | B) = [ Pr (A) × Pr (B) ] ÷ Pr (B) = Pr (A)

If event A and B are dependent, then Pr (A | B) ≠ Pr (A)

## Challenge 2: Deck of Cards

You have a deck of 52 cards, Ace to 10 + 3 face cards in each suit. You draw a card at random.

What is the probability that you draw a face card?
```{r Challenge2-1, echo=TRUE}
PrA <- 12/52
PrA
```

What is the probability that you draw a King?
```{r Challenge2-2, echo=TRUE}
PrA <- 4/52
PrA
```

What is the probability that you draw a spade?
```{r Challenge2-3, echo=TRUE}
PrA <- 1/4
PrA
```

What is the probability that you draw a spade given that you draw a face card? 

Pr (A | B) = Pr (A ⋂ B) ÷ Pr (B) or [ Pr (A) × Pr (B) ] ÷ Pr (B) = Pr (A). Since drawing a spade is independent from drawing a face card I will use this to show thatt Pr of A given B is the same as Pr(A)
```{r Challenge2-4, echo=TRUE}
PrB <- 12/52
PrB
PrA <- 1/4
PrA
PrAgB <- (PrA*PrB)/PrB
PrAgB
```

What is the probability that you draw a King given that you draw a face card?
Pr (A | B) = Pr (A ⋂ B) ÷ Pr (B) because these are dependent on each other it will not be equal to Pr(A)
```{r Challenge2-5, echo=TRUE}
PrB <- 12/52
PrB
PrA <- 4/12
PrA
PrAgB <- (PrA*PrB)
PrAgB
```

What is the probability that you draw a card that is both from a red suit (hearts or diamonds) and a face card?
Pr(A n B)
```{r Challenge2-6, echo=TRUE}
PrB <- 12/52
PrB
PrA <- 26/52
PrA
PrAnB <- (PrA*PrB)
PrAnB
```
What is the probability that you draw a card that is either a club or not a face card?
Pr (A ⋃ B) = Pr (A) + Pr (B) - Pr (A ⋂ B) (these are not mutually exclusive events)
```{r Challenge2-7, echo=TRUE}
PrA <- 1/4
PrA
PrB <- (52-12)/52
PrB
PrAuB <- PrA + PrB - (PrA*PrB)
PrAuB
```

## Random Variables

A random variable is a variable whose outcomes are assumed to arise by chance or according to some random or stochastic mechanism. The chances of observing a specific outcome or an outcome value within a specific interval has associated with it a probability.

Random variables come in two varieties:

Discrete Random Variables are random variables that can assume only a countable number of discrete possibilities (e.g., counts of outcomes in a particular category). We can assign a probability to each possible outcome.

Continuous Random Variables are random variables that can assume any real number value within a given range (e.g., measurements). We cannot assign a specific probability to each possible outcome value as the set of possible outcomes is infinite, but we can assign probabilities to intervals of outcome values.

With these basics in mind, we can define a few more terms:

A probability function is a mathematical function that describes the chance associated with a random variable having a particular outcome or falling within a given range of outcome values.

We can also distinguish two types of probability functions:

1. **Probability Mass Functions** (PMFs) are associated with *discrete random variables*. These functions describe the probability that a random variable takes a particular discrete value.

To be a valid PMF, a function f(x) must satisfy the following:

1.There are k distinct outcomes x1,x2,...,xk
2. 0 ≤ Pr (X=xi) ≤ 1 for all xi
3. ∑ Pr (X=xi) for all x from x1 to xk = 1

### Flip a Fair Coin

```{r RandomVars-1, echo=TRUE}
outcomes <- c("heads", "tails")
prob <- c(1/2, 1/2)
barplot(prob, ylim = c(0, 0.6), names.arg = outcomes, space = 0.1, xlab = "outcome",
    ylab = "Pr(X = outcome)", main = "Probability Mass Function")
```
```{r RandomVars-2, echo=TRUE}
cumprob <- cumsum(prob)
barplot(cumprob, names.arg = outcomes, space = 0.1, xlab = "outcome", ylab = "Cumulative Pr(X)",
    main = "Cumulative Probability")
```
### Rolling a Fair Die

```{r RandomVars-3, echo=TRUE}
outcomes <- c(1, 2, 3, 4, 5, 6)
prob <- c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)
barplot(prob, ylim = c(0, 0.5), names.arg = outcomes, space = 0.1, xlab = "outcome",
    ylab = "Pr(X = outcome)", main = "Probability Mass Function")
```

```{r RandomVars-4, echo=TRUE}
cumprob <- cumsum(prob)
barplot(cumprob, names.arg = outcomes, space = 0.1, xlab = "outcome", ylab = "Cumulative Pr(X)",
    main = "Cumulative Probability")
```

2. **Probability Density Functions (PDFs)** are associated with *continuous random variables*. These functions describe the probability that a random variable falls within a given range of outcome values. The probability associated with that range equals the area under the density function for that range.

"If I throw a rock what's the chance it falls within that hula hoop"

To be a valid PDF, a function f(x) must satisfy the following:

1. f(x) ≥ 0 for all −∞ ≤ x ≤ +∞. That is, the function f(x) is non-negative everywhere.
2. ∫+∞−∞ f(x) dx = 1. That is, the total area under the function f(x) = 1


### Beta Distribution
The Beta Distribution refers to a family of continuous probability distributions defined over the interval [0, 1] and parametrized by two positive shape parameters, denoted by α and β, that appear as exponents of the random variable x and control the shape of the distribution.

f(x) = K xα−1(1−x)β−1

If we set K = 2, α = 2, and β = 1 and restrict the domain of x to [0, 1], it gives us a triangular function that we can graph as follows:

```{r RandomVars-5, echo=TRUE}
a <- 2
b <- 1
K <- 2
x <- seq(from = 0, to = 1, by = 0.025)
fx <- K * x^(a - 1) * (1 - x)^(b - 1)
lower_x <- seq(from = -0.25, to = 0, by = 0.025)  # add some values of x less than zero
upper_x <- seq(from = 1, to = 1.25, by = 0.025)  # add some values of x greater than one
lower_fx <- rep(0, 11)  # add fx=0 values to x<0
upper_fx <- rep(0, 11)  # add fx=0 values to x>1
x <- c(lower_x, x, upper_x)  # paste xs together
fx <- c(lower_fx, fx, upper_fx)  # paste fxs together
d <- as.data.frame(cbind(x, fx))
p <- ggplot(data = d, aes(x = x, y = fx)) + xlab("x") + ylab("f(x)") + geom_line()
p
```

Is this a PDF? 
Does it satisfy x being between negative infinity and positive infinity?
Is the total area under the line = 1?

```{r RandomVars-6, echo=TRUE}
manipulate(ggplot(data = d, aes(x = x, y = fx)) + xlab("x") + ylab("f(x)") +
    geom_line() + geom_polygon(data = data.frame(xvals = c(0, n, n, 0), fxvals = c(0,
    K * n^(a - 1) * (1 - n)^(b - 1), 0, 0)), aes(x = xvals, y = fxvals)) + ggtitle(paste("Area Under Function = ",
    0.5 * n * K * n^(a - 1) * (1 - n)^(b - 1), sep = " ")), n = slider(0, 1,
    initial = 0.5, step = 0.01))
```

The shaded area here represents the cumulative probability integrated across f(x) from −inf to x.

The cumulative distribution function, or CDF, of a random variable is defined as the probability of observing a random variable X taking the value of x or less, i.e., F(x) = Pr (X ≤ x).

This definition applies regardless of whether X is discrete or continuous. Note here we are using F(x) for the cumulative distribution function rather than f(x), which we use for the probability density function. For a continuous variable, the PDF is simply the first derivative of the CDF, i.e., $f(x) = d F(x)

```{r RandomVars-7, echo=TRUE}
x <- seq(from = 0, to = 1, by = 0.005)
prob <- 0.5 * x * K * x^(a - 1) * (1 - x)^(b - 1)
barplot(prob, names.arg = x, space = 0, main = "Cumulative Probability", xlab = "x",
    ylab = "Pr(X ≤ x)")
```

Can use pbeta() function built in if we define values for alpha and beta

```{r RandomVars-8, echo=TRUE}
pbeta(0.75, 2, 1)
pbeta(0.5, 2, 1)
```

pbeta is returning the area under the triangle

Other examples:
rbeta(), dbeta(), and qbeta() 
rbeta() draws random observations from a specfied beta distribution
dbeta() gives the point estimate of the beta density function at the value of the argument x
qbeta() is essentially the converse of pbeta(), i.e., it tells you the value of x that is associated with a particular cumulative probability, or quantile, of the cumulative distribution function. Other PMFs and PDFs have comparable r, d, p, and q functions.

```{r RandomVars-9, echo=TRUE}
pbeta(0.7, 2, 1)
qbeta(0.49, 2, 1)
#each one of these will return the first one of the other
```

### Expected Mean and Variance of Random Variables

The mean value (or expectation) and the expected variance for a random variable with a given probability mass function can be expressed generally as follows:

μX = Expectation for X = ∑ xi × Pr (X=xi) for all x from xi to xk

σ2X = Variance of X = ∑ (xi−μX)2 × Pr (X=xi) for all x from xi to xk

Applying these formulae to die rolls, we could calculate the expectation for X for a large set of die rolls…

(1 * 1/6) + (2 * 1/6) + … + (6 * 1/6) = 3.5

this is a little more than half interestingly

Expectation:
```{r RandomVars-10, echo=TRUE}
m <- sum(seq(1:6)*1/6) #sequence of 1:6 each multiplied by 1/6 added up
m
```

and expected variance:
```{r RandomVars-11, echo=TRUE}
var <- sum((seq(1:6) - mean(seq(1:6)))^2 * (1/6))
var
```

## Useful Probability Distributions for Random Variables

The **Bernoulli Distribution** is the probability distribution of a binary random variable, i.e., a variable that has only two possible outcomes, such as success or failure, heads or tails, true or false. If p is the probability of one outcome, then 1−p has to be the probabilty of the alternative. For flipping a fair coin, for example, p = 0.5 and 1−p also = 0.5.

For the BERNOULLI DISTRIBUTION, the probability mass function is:
f(x) = p^x*(1−p)^1−x where x = {0 or 1}
For this distribution, μX = p and σ2X = p(1−p)

## Challenge 3: Bernoulli

Using the Bernoulli distribution, calculate the expectation for drawing a spade from a deck of cards? What is the variance in this expectation across a large number of draws?

```{r Challenge3-1, echo=TRUE}
PrA <- ((13/52)^1)*(39/52)^0
PrA
varPrA <- (13/52)*(1-(13/52)) #PrA * the complement of probability A
varPrA
```

The Bernoulli distribution is a special case of the Binomial Distribution. The binomial distribution is typically used to model the probability of a number of “successes” k out of a set of “trials” n, i.e., for counts of a particular outcome.

Again, the probability of success on each trial = p and the probability of not success = 1−p.

**See rule 6 of probability rules**

See Module 8 instruactions for formulae for binomal distribution (n choose k stuff)

## Challenge 4: Binomial Distribution

What is the chance of getting a “1” on each of six consecutive rolls of a die?

```{r Challenge4-1, echo=TRUE}
n <- 6  # number of trials
k <- 6  # number of successes
p <- 1/6
prob <- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n - k) 
# (6!(trials)/6!(succ) * 6-6!) * 6^6 * (1-1/6)^(6-6)
prob
```

What about of getting exactly three “1”s? On 6 rolls

```{r Challenge4-2, echo=TRUE}
n <- 6  # number of trials
k <- 3  # number of successes
p <- 1/6
prob <- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n - k) 
# (6!(trials)/6!(succ) * 6-6!) * 6^6 * (1-1/6)^(6-6)
prob
```

Can also use built in function in r

```{r Challenge4-3, echo=TRUE}
dbinom(x = k, size = n, prob = p)
```
 
 and can use pbinom() to see the probability of observing up to and including a given number of successes in n
 trials
 
 This is how many times you could see 0 to 6 "1's" after rolling a die 6 times. 
 
```{r Challenge4-4, echo=TRUE}
probset <- dbinom(x = 0:6, size = 6, prob = 1/6)  # x is number of successes, size is number of trials
barplot(probset, names.arg = 0:6, space = 0, xlab = "outcome", ylab = "Pr(X = outcome)",
    main = "Probability Mass Function")
cumprob = cumsum(probset)
barplot(cumprob, names.arg = 0:6, space = 0.1, xlab = "outcome", ylab = "Cumulative Pr(X)",
    main = "Cumulative Probability")
sum(probset)
```

the chance of getting exactly 3 "1s" (dbinom) is different from getting up to and including 3 "1s"(qbinom)

```{r Challenge4-5, echo=TRUE}
dbinom(x = 3, size = 6, prob = 1/6)
pbinom(q = 3, size = 6, prob = 1/6)
sum(dbinom(x = 0:3, size = 6, prob = 1/6)) #same as pbinom
```

and the chance of getting more than 3 "1s" in 6 rolls would be expressed with the negative binomial distribution pnbinom

```{r Challenge4-5, echo=TRUE}
1 - pnbinom(q = 3, size = 6, prob = 1/6)
pnbinom(q = 3, size = 6, prob = 1/6, lower.tail = FALSE)
```

and once again this is different from *3 or more* rolls

```{r Challenge4-6, echo=TRUE}
1 - pbinom(q = 2, size = 6, prob = 1/6)
pbinom(q = 2, size = 6, prob = 1/6, lower.tail = FALSE)
```


What is the expected number of “1”s to occur in six consecutive rolls?

Cannot figure this out or what is needed to figure this out

### Poisson Distribution

The Poisson Distribution is often used to model open ended counts of independently occurring events, for example the number of cars that pass a traffic intersection over a given interval of time or the number of times a monkey scratches itself during a given observation interval. The probability mass function for the Poisson distribution is described by a single parameter, λ, where λ can be interpreted as the mean number of occurrences of the event in the given interval.

See module directions for formula

Note that the mean and the variance are the same!

```{r Poisson-1, echo=TRUE}
x <- 0:10
l = 3.5
probset <- dpois(x = x, lambda = l)
barplot(probset, names.arg = x, space = 0, xlab = "x", ylab = "Pr(X = x)", main = "Probability Mass Function")
x <- 0:20
l = 10
probset <- dpois(x = x, lambda = l)
barplot(probset, names.arg = x, space = 0, xlab = "x", ylab = "Pr(X = x)", main = "Probability Mass Function")
x <- 0:50
l = 20
probset <- dpois(x = x, lambda = l)
barplot(probset, names.arg = x, space = 0, xlab = "x", ylab = "Pr(X = x)", main = "Probability Mass Function")
```
As we did for other distributions, we can also use the built in probability function for the Poisson distribution, ppois(), to return the value of the cumulative distribution function, i.e., the probability of observing up to and including a specific number of events in the given interval

```{r Poisson-2, echo=TRUE}
x <- 0:10
l <- 3.5
barplot(ppois(q = x, lambda = l), ylim = 0:1, space = 0, names.arg = x, xlab = "x",
    ylab = "Pr(X ≤ x)", main = "Cumulative Probability")
x <- 0:20
l <- 10
barplot(ppois(q = x, lambda = l), ylim = 0:1, space = 0, names.arg = x, xlab = "x",
    ylab = "Pr(X ≤ x)", main = "Cumulative Probability")
x <- 0:50
l <- 20
barplot(ppois(q = x, lambda = l), ylim = 0:1, space = 0, names.arg = x, xlab = "x",
    ylab = "Pr(X ≤ x)", main = "Cumulative Probability")
```

Every Saturday, at the same time, a primatologist goes and sits in the forest in the morning and listens for titi monkey calls, counting the number of calls they hear in a 2 hour window from 5am to 7am. Based on previous knowledge, she believes that the mean number calls she will hear in that time is exactly 15. Let X represent the appropriate Poisson random variable of the number of calls heard in each monitoring session.

What is the probability that she will hear more than 8 calls during any given session?
What is the probability that she will hear no calls in a session?
What is the probability that she will hear exactly 3 calls in a session?
Plot the relevant Poisson mass function over the values in range 0 ≤ x ≤ 30.
Simulate 104 results from this distribution (i.e., 2 years of Saturday monitoring sessions).
Plot the simulated results using hist() and use xlim() to set the horizontal limits to be from 0 to 30. How does your histogram compare to the shape of the probability mass function you plotted above?

```{r Poisson-2, echo=TRUE}
l1 <- 15^8
e <- exp(-8)
f <- factorial(8)
(l1*e)/f
Peight <- ((15^8)*exp(-8))/factorial(8)
Peight
#why does this return a number larger than 1?
l <- 15
x <- 0:30
calls <- dpois(x = x, lambda = l)
barplot(calls, names.arg = x, space = 0, xlab = "Titi Calls", ylab = "Probability of # of Calls", main = "Probability Mass Function of Titi Calls")
mornings <- 104
mornings * l #for each session 
```